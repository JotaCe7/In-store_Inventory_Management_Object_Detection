{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Detection Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets evaluate our model in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/home/app/src/data/yolo_data_v1.yaml, weights=['/home/app/src/experiments/exp/weights/best.pt'], batch_size=16, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=test, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=/home/app/src/evaluation/experiments, name=exp, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5 üöÄ v7.0-56-gc0ca1d2 Python-3.8.10 torch-1.10.0+cu111 CUDA:0 (Tesla K80, 11441MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 212 layers, 20852934 parameters, 0 gradients, 47.9 GFLOPs\n",
      "\u001b[34m\u001b[1mtest: \u001b[0mScanning /home/app/src/data/labels/test... 2920 images, 0 backgrounds, 0 c\u001b[0m\n",
      "\u001b[34m\u001b[1mtest: \u001b[0mNew cache created: /home/app/src/data/labels/test.cache\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2920     429411      0.925      0.835      0.904      0.565\n",
      "Speed: 0.3ms pre-process, 31.7ms inference, 6.9ms NMS per image at shape (16, 3, 640, 640)\n",
      "Results saved to \u001b[1m/home/app/src/evaluation/experiments/exp\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 /home/app/yolov5/val.py --data /home/app/src/data/yolo_data_v1.yaml --weights /home/app/src/experiments/exp/weights/best.pt --task='test'  --project /home/app/src/evaluation/experiments --name='exp' --batch 16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Precission-Recall Curve](../evaluation/experiments/exp/PR_curve.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Confusion Matric](../evaluation/experiments/exp/confusion_matrix.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product and Missing Product Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/home/app/src/datav2/data.yaml, weights=['/home/app/src/experiments/exp2/weights/best.pt'], batch_size=16, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=test, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=/home/app/src/evaluation/experiments, name=exp2, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5 üöÄ v7.0-56-gc0ca1d2 Python-3.8.10 torch-1.10.0+cu111 CUDA:0 (Tesla K80, 11441MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 212 layers, 20865057 parameters, 0 gradients, 47.9 GFLOPs\n",
      "\u001b[34m\u001b[1mtest: \u001b[0mScanning /home/app/src/datav2/test/labels.cache... 232 images, 0 backgroun\u001b[0m\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING ‚ö†Ô∏è NMS time limit 1.300s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        232      34595      0.875      0.745      0.798      0.476\n",
      "               missing        232       1255      0.824      0.631      0.688      0.364\n",
      "               product        232      33340      0.926      0.859      0.909      0.588\n",
      "Speed: 0.3ms pre-process, 32.9ms inference, 18.0ms NMS per image at shape (16, 3, 640, 640)\n",
      "Results saved to \u001b[1m/home/app/src/evaluation/experiments/exp25\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 /home/app/yolov5/val.py --data /home/app/src/datav2/data.yaml --weights /home/app/src/experiments/exp2/weights/best.pt --task='test'  --project /home/app/src/evaluation/experiments --name='exp2' --batch 16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Precission-Recall Curve](../evaluation/experiments/exp2/PR_curve.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Confusion Matric](../evaluation/experiments/exp2/confusion_matrix.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
